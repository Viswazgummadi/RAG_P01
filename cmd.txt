srun --partition=gpu --gres=gpu:1 --nodelist=node1 --pty bash -i


source ~/miniconda3/etc/profile.d/conda.sh
conda activate rag_env
python preprocess.py --input_dir data --output_file processed_data/processed_data.jsonl
 python vector_db.py --jsonl_file processed_data/processed_data.jsonl --output_dir vector_db
python scripts/prepare_training_data.py --jsonl_file processed_data/processed_data.jsonl --output_file training_data/training_data.jsonl --num_examples 5
 python scripts/train_model.py     --dataset_file training_data/training_data.jsonl     --model_name mistralai/Mistral-7B-v0.1     --output_dir models/checkpoints     --batch_size 1     --grad_accumulation 16     --epochs 1


python -c "import nltk; nltk.download('punkt'); nltk.download('punkt_tab')"
python scripts/evaluate_model.py     --model_path models/checkpoints/final     --test_file training_data/training_data.jsonl     --output_file models/evaluation_results.json     --use_lora     --base_model mistralai/Mistral-7B-v0.1     --test_size 5



python scripts/inference_pure.py --model_path models/checkpoints/final --use_lora --base_model "mistralai/Mistral-7B-v0.1"




# to compute database for all docs in data2 folder 

 python scripts/rebuild_vector_db.py --input_dir data2 --output_jsonl processed_data/all_documents.jsonl --vector_db_dir vector_db --chunk_size 800 --chunk_overlap 150


# Check how many documents were processed
wc -l processed_data/all_documents.jsonl

# Examine the first document to ensure it's formatted correctly
head -n 1 processed_data/all_documents.jsonl | python -m json.tool

# Check that the vector database was created
ls -la vector_db/

# First, create the vector_db directory
mkdir -p vector_db

# Then run vector_db.py with your processed data
python vector_db.py \
    --jsonl_file processed_data/all_processed.jsonl \
    --output_dir vector_db \
    --chunk_size 1000 \
    --chunk_overlap 200









# Process all documents
python preprocess.py --input_dir data2 --output_file processed_data/all_processed.jsonl

# Create clean training data
python scripts/create_clean_data.py --input_file processed_data/all_processed.jsonl --output_file training_data/full_training.jsonl --num_examples 5

# Train with a pre-tuned instruction model
python scripts/train_model.py \
    --dataset_file training_data/full_training.jsonl \
    --model_name "mistralai/Mistral-7B-Instruct-v0.2" \
    --output_dir models/instruct_model \
    --batch_size 1 \
    --grad_accumulation 16 \
    --epochs 3 \
    --use_lora







the latest rag is working good 

but working only based on vector db it now have only file should update it with all files data :)
python scripts/test_base_model.py --vector_store_path vector_db





# Evaluate fine-tuned model with RAG
python scripts/compare_models.py \
    --finetuned_model models/instruct_model/final \
    --base_model "mistralai/Mistral-7B-Instruct-v0.2" \
    --use_lora \
    --compare_with_base \
    --use_rag \
    --vector_store_path vector_db \
    --output_file models/evaluation_comparison.json




# rest with gradio
python scripts/model_comparison_ui.py \
    --base_model "mistralai/Mistral-7B-Instruct-v0.2" \
    --finetuned_model models/instruct_model/final \
    --use_lora \
    --vector_store vector_db




#to test the base model and RAG combined use this cmd 
python scripts/inference_rag.py     --model_path "mistralai/Mistral-7B-Instruct-v0.2"     --vector_store_path vector_db     --k_docs 3
